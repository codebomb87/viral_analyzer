"""
í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¶„ì„ ëª¨ë“ˆ
"""

import re
import pandas as pd
import numpy as np
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import config

# matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •
try:
    # Windows
    if 'malgun' in [f.name.lower() for f in fm.fontManager.ttflist]:
        plt.rcParams['font.family'] = 'Malgun Gothic'
    # macOS
    elif 'applesd gothic neo' in [f.name.lower() for f in fm.fontManager.ttflist]:
        plt.rcParams['font.family'] = 'AppleSD Gothic Neo'  
    # ê¸°ë³¸ê°’
    else:
        plt.rcParams['font.family'] = 'DejaVu Sans'
except:
    pass

plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ í°íŠ¸ ê¹¨ì§ ë°©ì§€

try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    from nltk.tag import pos_tag
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    nltk.download('averaged_perceptron_tagger', quiet=True)
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False

try:
    from konlpy.tag import Okt
    KONLPY_AVAILABLE = True
except ImportError:
    KONLPY_AVAILABLE = False

class KeywordAnalyzer:
    def __init__(self):
        self.korean_stopwords = set(config.STOPWORDS_KO)
        if NLTK_AVAILABLE:
            self.english_stopwords = set(stopwords.words('english')).union(set(config.STOPWORDS_EN))
        else:
            self.english_stopwords = set(config.STOPWORDS_EN)
        
        if KONLPY_AVAILABLE:
            self.okt = Okt()
        else:
            self.okt = None
    
    def extract_keywords_from_text(self, text, language='auto', max_keywords=20):
        """
        í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸
            language (str): ì–¸ì–´ ('ko', 'en', 'auto')
            max_keywords (int): ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜
        
        Returns:
            list: (í‚¤ì›Œë“œ, ë¹ˆë„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not text or not isinstance(text, str):
            return []
        
        # ì–¸ì–´ ìë™ ê°ì§€
        if language == 'auto':
            language = self.detect_language(text)
        
        if language == 'ko' and self.okt:
            keywords = self.extract_korean_keywords(text, max_keywords)
        else:
            keywords = self.extract_english_keywords(text, max_keywords)
        
        return keywords
    
    def detect_language(self, text):
        """
        í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
        
        Args:
            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸
        
        Returns:
            str: 'ko' ë˜ëŠ” 'en'
        """
        # í•œê¸€ ë¬¸ì ë¹„ìœ¨ ê³„ì‚°
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        total_chars = len(re.findall(r'[ê°€-í£a-zA-Z]', text))
        
        if total_chars == 0:
            return 'en'
        
        korean_ratio = korean_chars / total_chars
        return 'ko' if korean_ratio > 0.3 else 'en'
    
    def extract_korean_keywords(self, text, max_keywords=20):
        """
        í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        if not self.okt:
            return self.extract_simple_keywords(text, max_keywords)
        
        try:
            # í˜•íƒœì†Œ ë¶„ì„
            morphs = self.okt.morphs(text, stem=True)
            
            # ëª…ì‚¬ë§Œ ì¶”ì¶œ
            nouns = self.okt.nouns(text)
            
            # ë¶ˆìš©ì–´ ì œê±° ë° ê³ ê¸‰ í•„í„°ë§
            filtered_words = [
                word for word in nouns 
                if (len(word) > 1 and 
                    word not in self.korean_stopwords and
                    not word.isdigit() and
                    not self.is_url_fragment(word.lower()) and
                    not self.is_meaningless_word(word.lower()))
            ]
            
            # ë¹ˆë„ ê³„ì‚°
            word_freq = Counter(filtered_words)
            
            return word_freq.most_common(max_keywords)
            
        except Exception as e:
            print(f"í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return self.extract_simple_keywords(text, max_keywords)
    
    def extract_english_keywords(self, text, max_keywords=20):
        """
        ì˜ì–´ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        if not NLTK_AVAILABLE:
            return self.extract_simple_keywords(text, max_keywords)
        
        try:
            # í† í°í™”
            tokens = word_tokenize(text.lower())
            
            # í’ˆì‚¬ íƒœê¹…
            pos_tags = pos_tag(tokens)
            
            # ëª…ì‚¬ì™€ í˜•ìš©ì‚¬ë§Œ ì¶”ì¶œ (ê³ ê¸‰ í•„í„°ë§ ì ìš©)
            keywords = [
                word for word, pos in pos_tags
                if (pos.startswith(('NN', 'JJ')) and len(word) > 2
                    and word not in self.english_stopwords
                    and word.isalpha()
                    and not self.is_url_fragment(word.lower())
                    and not self.is_meaningless_word(word.lower()))
            ]
            
            # ë¹ˆë„ ê³„ì‚°
            word_freq = Counter(keywords)
            
            return word_freq.most_common(max_keywords)
            
        except Exception as e:
            print(f"ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return self.extract_simple_keywords(text, max_keywords)
    
    def extract_simple_keywords(self, text, max_keywords=20):
        """
        ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´)
        """
        # URL íŒ¨í„´ ì œê±°
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        text = re.sub(r'www\.[\w.-]+\.\w+', '', text)
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬ (í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ìœ ì§€)
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        words = text.split()
        
        # ê³ ê¸‰ í•„í„°ë§
        filtered_words = []
        for word in words:
            word_lower = word.lower()
            # ê¸¸ì´, ìˆ«ì, ë¶ˆìš©ì–´ í•„í„°ë§
            if (len(word) > 2 and 
                not word.isdigit() and 
                not word_lower in self.korean_stopwords and 
                not word_lower in self.english_stopwords and
                not self.is_url_fragment(word_lower) and
                not self.is_meaningless_word(word_lower)):
                filtered_words.append(word_lower)
        
        # ë¹ˆë„ ê³„ì‚°
        word_freq = Counter(filtered_words)
        
        return word_freq.most_common(max_keywords)
    
    def is_url_fragment(self, word):
        """URL ì¡°ê°ì¸ì§€ í™•ì¸"""
        url_patterns = [
            r'^www',
            r'\.com$',
            r'\.net$',
            r'\.org$',
            r'\.kr$',
            r'\.co$',
            r'^http',
            r'^https',
            r'youtube',
            r'youtu\.be'
        ]
        
        for pattern in url_patterns:
            if re.search(pattern, word):
                return True
        return False
    
    def is_meaningless_word(self, word):
        """ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ì¸ì§€ í™•ì¸"""
        # ì—°ì†ëœ ê°™ì€ ë¬¸ì (ã…‹ã…‹ã…‹, ã…ã…ã… ë“±)
        if re.match(r'^(.)\1{2,}$', word):
            return True
        
        # íŠ¹ìˆ˜ë¬¸ìë§Œ í¬í•¨ëœ ë‹¨ì–´
        if re.match(r'^[^\wê°€-í£]+$', word):
            return True
        
        # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë‹¨ì–´
        if len(word) < 2 or len(word) > 20:
            return True
            
        return False
    
    def analyze_video_keywords(self, videos_data):
        """
        ì—¬ëŸ¬ ì˜ìƒì˜ í‚¤ì›Œë“œë¥¼ ì¢…í•© ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            videos_data (list): ì˜ìƒ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        
        Returns:
            dict: í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼
        """
        all_titles = []
        all_descriptions = []
        all_tags = []
        
        for video in videos_data:
            all_titles.append(video.get('title', ''))
            all_descriptions.append(video.get('description', ''))
            if video.get('tags'):
                all_tags.extend(video['tags'])
        
        # ì œëª© í‚¤ì›Œë“œ ë¶„ì„
        title_text = ' '.join(all_titles)
        title_keywords = self.extract_keywords_from_text(title_text, max_keywords=30)
        
        # ì„¤ëª… í‚¤ì›Œë“œ ë¶„ì„
        description_text = ' '.join(all_descriptions)
        description_keywords = self.extract_keywords_from_text(description_text, max_keywords=30)
        
        # íƒœê·¸ ë¹ˆë„ ë¶„ì„
        tag_freq = Counter(all_tags)
        top_tags = tag_freq.most_common(20)
        
        # ì „ì²´ í‚¤ì›Œë“œ í†µí•©
        all_keywords = {}
        for keyword, freq in title_keywords:
            all_keywords[keyword] = all_keywords.get(keyword, 0) + freq * 3  # ì œëª© ê°€ì¤‘ì¹˜ 3ë°°
        
        for keyword, freq in description_keywords:
            all_keywords[keyword] = all_keywords.get(keyword, 0) + freq
        
        for tag, freq in top_tags:
            all_keywords[tag] = all_keywords.get(tag, 0) + freq * 2  # íƒœê·¸ ê°€ì¤‘ì¹˜ 2ë°°
        
        # ìƒìœ„ í‚¤ì›Œë“œ ì„ ë³„
        top_keywords = sorted(all_keywords.items(), key=lambda x: x[1], reverse=True)[:50]
        
        return {
            'title_keywords': title_keywords,
            'description_keywords': description_keywords,
            'top_tags': top_tags,
            'combined_keywords': top_keywords,
            'total_videos_analyzed': len(videos_data)
        }
    
    def create_wordcloud(self, keywords_freq, width=400, height=200, 
                        background_color='white', max_words=50):
        """
        í‚¤ì›Œë“œë¡œ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            keywords_freq (list): (í‚¤ì›Œë“œ, ë¹ˆë„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            width (int): ì›Œë“œí´ë¼ìš°ë“œ ë„ˆë¹„
            height (int): ì›Œë“œí´ë¼ìš°ë“œ ë†’ì´
            background_color (str): ë°°ê²½ìƒ‰
            max_words (int): ìµœëŒ€ ë‹¨ì–´ ìˆ˜
        
        Returns:
            matplotlib.figure.Figure: ì›Œë“œí´ë¼ìš°ë“œ ê·¸ë¦¼
        """
        if not keywords_freq:
            fig, ax = plt.subplots(figsize=(6, 3))
            ax.text(0.5, 0.5, 'í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤', ha='center', va='center', 
                   fontsize=16, transform=ax.transAxes)
            ax.set_xticks([])
            ax.set_yticks([])
            return fig
        
        # í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        keyword_dict = dict(keywords_freq)
        
        # í•œê¸€ í°íŠ¸ ê²½ë¡œ ì°¾ê¸°
        try:
            from font_utils import get_wordcloud_font_path
            font_path = get_wordcloud_font_path()
        except Exception as e:
            print(f"í°íŠ¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            font_path = None
        
        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹œë„
        wordcloud_params = {
            'width': width,
            'height': height,
            'background_color': background_color,
            'max_words': max_words,
            'relative_scaling': 0.5,
            'min_font_size': 8,
            'colormap': 'viridis'
        }
        
        # í°íŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if font_path:
            wordcloud_params['font_path'] = font_path
        
        try:
            # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
            wordcloud = WordCloud(**wordcloud_params).generate_from_frequencies(keyword_dict)
            
            # ê·¸ë˜í”„ ìƒì„±
            fig, ax = plt.subplots(figsize=(6, 3))
            ax.imshow(wordcloud, interpolation='bilinear')
            ax.axis('off')
            
            return fig
            
        except Exception as e:
            print(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì˜¤ë¥˜: {e}")
            
            # í°íŠ¸ ì—†ì´ ë‹¤ì‹œ ì‹œë„
            try:
                if 'font_path' in wordcloud_params:
                    del wordcloud_params['font_path']
                wordcloud = WordCloud(**wordcloud_params).generate_from_frequencies(keyword_dict)
                
                fig, ax = plt.subplots(figsize=(6, 3))
                ax.imshow(wordcloud, interpolation='bilinear')
                ax.axis('off')
                
                print("âš ï¸ í•œê¸€ í°íŠ¸ ì—†ì´ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
                return fig
                
            except Exception as e2:
                print(f"í°íŠ¸ ì—†ëŠ” ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ë„ ì‹¤íŒ¨: {e2}")
                
                # ìµœì¢… ëŒ€ì²´: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ í‘œì‹œ
                fig, ax = plt.subplots(figsize=(6, 3))
                
                # ìƒìœ„ í‚¤ì›Œë“œë“¤ì„ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
                top_keywords = list(keyword_dict.keys())[:10]
                keyword_text = ", ".join(top_keywords)
                
                ax.text(0.5, 0.5, f'ì£¼ìš” í‚¤ì›Œë“œ:\n{keyword_text}', 
                       ha='center', va='center', fontsize=10, 
                       transform=ax.transAxes, wrap=True)
                ax.set_xticks([])
                ax.set_yticks([])
                ax.set_title('í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼')
                
                return fig
    
    def analyze_keyword_trends(self, videos_data, time_period='daily'):
        """
        ì‹œê°„ë³„ í‚¤ì›Œë“œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            videos_data (list): ì˜ìƒ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            time_period (str): ë¶„ì„ ê¸°ê°„ ('daily', 'weekly', 'monthly')
        
        Returns:
            dict: íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼
        """
        if not videos_data:
            return {}
        
        # ë‚ ì§œë³„ë¡œ ì˜ìƒ ê·¸ë£¹í™”
        videos_df = pd.DataFrame(videos_data)
        videos_df['published_date'] = pd.to_datetime(videos_df['published_at'])
        
        if time_period == 'daily':
            videos_df['time_group'] = videos_df['published_date'].dt.date
        elif time_period == 'weekly':
            videos_df['time_group'] = videos_df['published_date'].dt.to_period('W')
        else:  # monthly
            videos_df['time_group'] = videos_df['published_date'].dt.to_period('M')
        
        # ì‹œê°„ëŒ€ë³„ í‚¤ì›Œë“œ ë¶„ì„
        trend_data = {}
        for time_group, group_data in videos_df.groupby('time_group'):
            group_videos = group_data.to_dict('records')
            keywords_analysis = self.analyze_video_keywords(group_videos)
            trend_data[str(time_group)] = keywords_analysis['combined_keywords'][:10]
        
        return trend_data
    
    def get_keyword_insights(self, keyword_analysis):
        """
        í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ì—ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            keyword_analysis (dict): í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼
        
        Returns:
            list: ì¸ì‚¬ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        insights = []
        
        if not keyword_analysis.get('combined_keywords'):
            return ["ë¶„ì„í•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤."]
        
        top_keywords = keyword_analysis['combined_keywords'][:5]
        insights.append(f"ğŸ” ê°€ì¥ ì¸ê¸° ìˆëŠ” í‚¤ì›Œë“œ: {', '.join([kw[0] for kw in top_keywords])}")
        
        # ì œëª©ê³¼ ì„¤ëª…ì—ì„œ ê³µí†µìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œ ì°¾ê¸°
        title_kw = set([kw[0] for kw in keyword_analysis['title_keywords'][:10]])
        desc_kw = set([kw[0] for kw in keyword_analysis['description_keywords'][:10]])
        common_kw = title_kw.intersection(desc_kw)
        
        if common_kw:
            insights.append(f"ğŸ’¡ ì œëª©ê³¼ ì„¤ëª…ì— ê³µí†µìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(list(common_kw)[:3])}")
        
        # íƒœê·¸ í™œìš©ë„ ë¶„ì„
        if keyword_analysis.get('top_tags'):
            avg_tag_freq = sum([freq for _, freq in keyword_analysis['top_tags']]) / len(keyword_analysis['top_tags'])
            insights.append(f"ğŸ·ï¸ í‰ê·  íƒœê·¸ ì‚¬ìš© ë¹ˆë„: {avg_tag_freq:.1f}íšŒ")
        
        return insights
    
 